# app.py

# Imports
import os
import pandas as pd
import openai
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.tools import StructuredTool
from langchain.schema import SystemMessage
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# Additional imports for visualization
import matplotlib.pyplot as plt
from IPython.display import display, clear_output

# Initialize conversation history
if 'conversation_history' not in globals():
    conversation_history = []

# Initialize memory
if 'memory' not in globals():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Initialize the LLM with system prompt
if 'llm' not in globals():
    openai_api_key = os.environ.get("OPENAI_API_KEY")  # Replace with your method
    llm = ChatOpenAI(
        model_name="gpt-4-0613",  # Ensure function calling support
        temperature=0,
        openai_api_base=os.environ.get("gw_base_url"),
        openai_api_key=openai_api_key,
    )

# System prompt to guide the LLM
system_prompt = """
You are a helpful assistant for the corporate treasury department.

When a user asks a question, determine the type of question:

1. If the question is about 'haircut' (contains the keyword 'haircut'), collect necessary parameters and call the 'query_dataframe' function to get data.

2. If the question is about FX Rates (e.g., involves currencies, exchange rates, dates), collect necessary parameters and call the 'visualize_fx_rates' function to generate the requested visualization.

3. For other questions, use the 'retrieve_documents' function to search the corpus for relevant information, and provide an informative answer.

Always ensure that you are polite and helpful. If any parameters are missing for a function, ask the user for the missing information.
"""

# Load your DataFrames
if 'df_liquidity' not in globals():
    # Liquidity Data
    df_liquidity = pd.read_csv('liquidity_data.csv')  # Replace with your actual CSV file path
    df_liquidity['Liquidity Variable'] = df_liquidity['Liquidity Variable'].str.lower()

if 'df_fx_rates' not in globals():
    # FX Rates Data
    df_fx_rates = pd.read_csv('FX_Rates.csv')  # Replace with your actual CSV file path

    # Preprocess FX Rates Data
    # Melt the DataFrame to have dates in a single column
    date_columns = [col for col in df_fx_rates.columns if '-' in col]
    df_fx_rates_melted = df_fx_rates.melt(
        id_vars=['Currency', 'Name'],
        value_vars=date_columns,
        var_name='Date',
        value_name='FX Rate'
    )

    # Convert Date column to datetime
    df_fx_rates_melted['Date'] = pd.to_datetime(df_fx_rates_melted['Date'], format='%y-%b')

# Define the query_dataframe function
def query_dataframe(parameters: dict):
    # Extract parameters
    liquidity_variable = parameters.get('liquidity_variable')
    time_to_maturity = parameters.get('time_to_maturity')
    region = parameters.get('region')
    fi_nonfi = parameters.get('fi_nonfi')
    ig_nonig = parameters.get('ig_nonig')
    
    # Map time_to_maturity to the correct bucket
    ttm_mapping = {
        "less than 1 year": "< 1 year",
        "under 1 year": "< 1 year",
        "< 1 year": "< 1 year",
        "1 year": "< 1 year",
        "between 1 and 5 years": "1 - 5 years",
        "1 - 5 years": "1 - 5 years",
        "1 to 5 years": "1 - 5 years",
        "2 years": "1 - 5 years",
        "3 years": "1 - 5 years",
        "4 years": "1 - 5 years",
        "5 years": "1 - 5 years",
        "more than 5 years": "> 5 years",
        "> 5 years": "> 5 years",
        "over 5 years": "> 5 years",
        "6 years": "> 5 years",
    }
    if time_to_maturity:
        time_to_maturity = ttm_mapping.get(time_to_maturity.lower(), time_to_maturity)
    
    # Apply filters based on parameters
    query_df = df_liquidity.copy()
    
    # Apply Liquidity Variable filter
    if liquidity_variable:
        query_df = query_df[query_df['Liquidity Variable'].str.lower() == liquidity_variable.lower()]
    
    # Apply Time to Maturity filter
    if time_to_maturity:
        query_df = query_df[query_df['Time to Maturity'] == time_to_maturity]
    
    # Apply Region filter
    if region:
        query_df = query_df[query_df['Region'].str.upper() == region.upper()]
    
    # For certain Liquidity Variables, FI_NonFI and IG_NonIG are ignored
    special_liquidity_variables = [
        'uk government bonds', 'german government bonds', 'france government bonds',
        'au government bonds', 'jp government bonds', 'hk government bonds',
        'sg government bonds', 'canadian provinces - british columbia',
        'canadian provinces - ontario', 'canadian provinces - alberta',
        'canadian provinces - quebec'
    ]
    if liquidity_variable and liquidity_variable.lower() not in special_liquidity_variables:
        if fi_nonfi:
            query_df = query_df[query_df['FI_NonFI'] == fi_nonfi]
        if ig_nonig:
            query_df = query_df[query_df['IG_NonIG'] == ig_nonig]
    
    if query_df.empty:
        return "No data found for the given parameters."
    
    # Display the result DataFrame
    display(query_df)
    
    return "Here are the results based on your query."

# Define the visualize_fx_rates function
def visualize_fx_rates(parameters: dict):
    """
    Visualize FX Rates data based on user parameters.
    
    Expected parameters:
    - currencies: list of currency codes or names
    - start_period: start period as string (e.g., 'January')
    - end_period: end period as string (e.g., 'August')
    - comparison_type: 'difference', 'percentage difference', etc.
    - chart_type: 'line', 'bar', etc.
    """
    currencies = parameters.get('currencies')
    start_period = parameters.get('start_period')
    end_period = parameters.get('end_period')
    comparison_type = parameters.get('comparison_type', 'difference')
    chart_type = parameters.get('chart_type', 'line')

    # Validate and preprocess inputs
    if not currencies or not start_period or not end_period:
        return "Missing required parameters."
    
    # Convert periods to datetime
    try:
        start_date = pd.to_datetime(start_period, format='%B')
        end_date = pd.to_datetime(end_period, format='%B')
    except Exception:
        return "Invalid date format. Please provide month names like 'January', 'February', etc."
    
    # Filter data for the given currencies and date range
    df_filtered = df_fx_rates_melted[
        df_fx_rates_melted['Name'].str.contains('|'.join(currencies), case=False)
    ]
    
    # Pivot data for easier plotting
    df_pivot = df_filtered.pivot_table(
        index='Date', columns='Name', values='FX Rate'
    ).reset_index()
    
    # Filter dates
    df_pivot = df_pivot[(df_pivot['Date'].dt.month >= start_date.month) & (df_pivot['Date'].dt.month <= end_date.month)]
    
    if df_pivot.empty:
        return "No data available for the given parameters."
    
    # Perform comparison
    if comparison_type == 'difference':
        df_diff = df_pivot.iloc[-1, 1:] - df_pivot.iloc[0, 1:]
        title = f"Difference in FX Rates between {start_period} and {end_period}"
    elif comparison_type == 'percentage difference':
        df_diff = ((df_pivot.iloc[-1, 1:] - df_pivot.iloc[0, 1:]) / df_pivot.iloc[0, 1:]) * 100
        title = f"Percentage Difference in FX Rates between {start_period} and {end_period}"
    else:
        return "Invalid comparison type."
    
    # Plotting
    plt.figure(figsize=(10, 6))
    if chart_type == 'bar':
        df_diff.plot(kind='bar')
    else:
        df_diff.plot()
    
    plt.title(title)
    plt.ylabel('FX Rate')
    plt.xlabel('Currency')
    
    # Highlight the currency with the highest difference
    max_currency = df_diff.idxmax()
    max_value = df_diff.max()
    plt.annotate(f'Highest: {max_currency}', xy=(max_currency, max_value), xytext=(0, 10),
                 textcoords='offset points', ha='center', va='bottom', fontsize=12, color='red')
    
    # Display the plot
    plt.show()
    
    # Display the filtered data
    display(df_filtered)
    
    return "Here is the visualization as per your request."

# Set up the RAG chunking and retrieval system using Chroma DB
if 'vectorstores' not in globals():
    # Initialize the embeddings model
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    
    # Load documents
    loader = DirectoryLoader('documents', glob='*.txt')  # Replace 'documents' with your directory
    documents = loader.load()
    
    # Generate multiple sets of chunks for each document
    chunk_sizes = [1024, 512, 256, 128]
    vectorstores = {}
    
    for chunk_size in chunk_sizes:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        # Add doc_id and chunk_id to metadata
        for idx, doc in enumerate(docs):
            doc.metadata['doc_id'] = doc.metadata.get('source', 'unknown')
            doc.metadata['chunk_id'] = idx
            doc.metadata['chunk_size'] = chunk_size
        # Create Chroma vector store for this chunk size
        vectorstore = Chroma.from_documents(docs, embeddings, collection_name=f'docs_{chunk_size}')
        vectorstores[chunk_size] = vectorstore

# Define the retrieve_documents function
def retrieve_documents(query):
    # First request: get initial top K results from the smallest chunk size (e.g., 128)
    initial_chunk_size = 128
    target_fragment_size = 512  # or 1024
    top_k = 5  # Number of top results to retrieve

    vectorstore = vectorstores[initial_chunk_size]
    docs_and_scores = vectorstore.similarity_search_with_score(query, k=top_k)

    # For each top K result, retrieve adjacent chunks to match target fragment size
    # Collect augmented documents
    augmented_docs = []

    for doc, score in docs_and_scores:
        doc_id = doc.metadata['doc_id']
        chunk_id = doc.metadata['chunk_id']
        # Now, retrieve adjacent chunks from the vectorstore with larger chunk sizes
        # For simplicity, use the largest chunk size that matches target fragment size
        if target_fragment_size >= 1024 and 1024 in vectorstores:
            augment_chunk_size = 1024
        elif target_fragment_size >= 512 and 512 in vectorstores:
            augment_chunk_size = 512
        else:
            augment_chunk_size = initial_chunk_size

        augment_vectorstore = vectorstores[augment_chunk_size]
        # Retrieve all chunks from the same document
        filter_function = lambda doc: doc.metadata['doc_id'] == doc_id
        augment_docs = augment_vectorstore.similarity_search(
            query, k=top_k, filter=filter_function
        )

        # Combine chunks to match target fragment size
        combined_text = ''.join([d.page_content for d in augment_docs])
        augmented_doc = Document(page_content=combined_text, metadata={'doc_id': doc_id})
        augmented_docs.append(augmented_doc)

    # Return the augmented documents as a concatenated string
    retrieved_text = '\n'.join([doc.page_content for doc in augmented_docs])

    return retrieved_text

# Define the tools
tools = [
    StructuredTool.from_function(
        func=query_dataframe,
        name="query_dataframe",
        description="Queries the liquidity DataFrame based on parameters.",
    ),
    StructuredTool.from_function(
        func=visualize_fx_rates,
        name="visualize_fx_rates",
        description="Visualizes FX Rates data based on user parameters.",
    ),
    StructuredTool.from_function(
        func=retrieve_documents,
        name="retrieve_documents",
        description="Retrieves relevant documents from the corpus.",
    )
]

# Initialize the agent with the system prompt
if 'agent' not in globals():
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.OPENAI_FUNCTIONS,
        verbose=True,
        memory=memory,
        agent_kwargs={"system_message": SystemMessage(content=system_prompt)}
    )

# Get user input from the Databricks widget
dbutils.widgets.text("Text Box", "", "Enter your query here")
user_input = dbutils.widgets.get("Text Box")

if user_input:
    # Append user's message to conversation history
    conversation_history.append({"role": "user", "content": user_input})
    
    # Update memory with conversation history
    # Note: memory is automatically updated in LangChain
    
    # Run the agent
    print("Assistant is typing...")
    assistant_response = agent.run(
        input=user_input
    )
    
    # Append assistant's response to conversation history
    conversation_history.append({"role": "assistant", "content": assistant_response})
    
    # Clear the widget value for the next input
    dbutils.widgets.remove("Text Box")
    dbutils.widgets.text("Text Box", "", "Enter your query here")
    
    # Display the assistant's response
    print("\nAssistant:")
    print(assistant_response)
    
    # Display the conversation history (optional)
    # for message in conversation_history:
    #     role = message['role'].capitalize()
    #     content = message['content']
    #     print(f"{role}: {content}")
