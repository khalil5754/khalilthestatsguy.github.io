# app.py

# Imports
import os
import pandas as pd
import openai
from langchain.chat_models import ChatOpenAI
from langchain.agents import AgentExecutor, Tool
from langchain.memory import ConversationBufferMemory
from langchain.tools import StructuredTool
from langchain.schema import SystemMessage
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
# Removed the import of BaseRetriever
# from langchain.retrievers import BaseRetriever

from langchain.agents import OpenAIFunctionsAgent
from pydantic import BaseModel, Field
from typing import Optional, List

# Additional imports for visualization
import matplotlib.pyplot as plt
from IPython.display import display, clear_output

# Initialize conversation history
if 'conversation_history' not in globals():
    conversation_history = []

# Initialize memory
if 'memory' not in globals():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# Initialize the LLM with system prompt
if 'llm' not in globals():
    openai_api_key = os.environ.get("OPENAI_API_KEY")  # Replace with your method
    llm = ChatOpenAI(
        model_name="gpt-4-0613",  # Ensure function calling support
        temperature=0,
        openai_api_key=openai_api_key,
    )

# System prompt to guide the LLM
system_prompt = """
You are a helpful assistant for the corporate treasury department.

When a user asks a question, determine the type of question:

1. If the question is about 'haircut' (contains the keyword 'haircut'), collect necessary parameters and call the 'query_dataframe' function to get data.

2. If the question is about FX Rates (e.g., involves currencies, exchange rates, dates), collect necessary parameters and call the 'visualize_fx_rates' function to generate the requested visualization.

3. For other questions, use the 'retrieve_documents' function to search the corpus for relevant information, and provide an informative answer.

Always ensure that you are polite and helpful. If any parameters are missing for a function, ask the user for the missing information.
"""

# Load your DataFrames
if 'df_liquidity' not in globals():
    # Liquidity Data
    df_liquidity = pd.read_csv('liquidity_data.csv')  # Replace with your actual CSV file path
    df_liquidity['Liquidity Variable'] = df_liquidity['Liquidity Variable'].str.lower()

if 'df_fx_rates' not in globals():
    # FX Rates Data
    df_fx_rates = pd.read_csv('FX_Rates.csv')  # Replace with your actual CSV file path

    # Preprocess FX Rates Data
    # Melt the DataFrame to have dates in a single column
    date_columns = [col for col in df_fx_rates.columns if '-' in col]
    df_fx_rates_melted = df_fx_rates.melt(
        id_vars=['Currency', 'Name'],
        value_vars=date_columns,
        var_name='Date',
        value_name='FX Rate'
    )

    # Convert Date column to datetime
    df_fx_rates_melted['Date'] = pd.to_datetime(df_fx_rates_melted['Date'], format='%y-%b')

# Define input schemas using Pydantic models

class QueryDataFrameInput(BaseModel):
    liquidity_variable: str = Field(description="The liquidity variable.")
    time_to_maturity: str = Field(description="The time to maturity.")
    region: str = Field(description="The region.")
    fi_nonfi: Optional[str] = Field(default=None, description="FI or NonFI.")
    ig_nonig: Optional[str] = Field(default=None, description="IG or NonIG.")

def query_dataframe(input: QueryDataFrameInput) -> str:
    # Function implementation as previously provided
    # Extract parameters
    liquidity_variable = input.liquidity_variable
    time_to_maturity = input.time_to_maturity
    region = input.region
    fi_nonfi = input.fi_nonfi
    ig_nonig = input.ig_nonig

    # Rest of the function logic
    # ...

class VisualizeFXRatesInput(BaseModel):
    currencies: List[str] = Field(description="List of currency codes or names.")
    start_period: str = Field(description="Start period (e.g., 'January').")
    end_period: str = Field(description="End period (e.g., 'August').")
    comparison_type: Optional[str] = Field(default='difference', description="'difference' or 'percentage difference'.")
    chart_type: Optional[str] = Field(default='line', description="'line' or 'bar'.")

def visualize_fx_rates(input: VisualizeFXRatesInput) -> str:
    # Function implementation as previously provided
    # ...

def retrieve_documents(query: str) -> str:
    # Use the hybrid_chain to retrieve documents
    result = hybrid_chain({"query": query})['result']
    return result

# Set up the RAG chunking and retrieval system using Chroma DB
if 'vectorstores' not in globals():
    # Initialize the embeddings model
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Load documents
    loader = DirectoryLoader('documents', glob='*.txt')  # Replace 'documents' with your directory
    documents = loader.load()

    # Generate multiple sets of chunks for each document
    chunk_sizes = [1024, 512, 256, 128]
    vectorstores = {}

    for chunk_size in chunk_sizes:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        # Add doc_id and chunk_id to metadata
        for idx, doc in enumerate(docs):
            doc.metadata['doc_id'] = doc.metadata.get('source', 'unknown')
            doc.metadata['chunk_id'] = idx
            doc.metadata['chunk_size'] = chunk_size
        # Create Chroma vector store for this chunk size
        vectorstore = Chroma.from_documents(docs, embeddings, collection_name=f'docs_{chunk_size}')
        vectorstores[chunk_size] = vectorstore

# Define a custom retriever without inheriting from BaseRetriever
class CustomHybridRetriever:
    def __init__(self, vectorstores, initial_chunk_size=128, target_fragment_size=512, top_k=5):
        self.vectorstores = vectorstores
        self.initial_chunk_size = initial_chunk_size
        self.target_fragment_size = target_fragment_size
        self.top_k = top_k

    def get_relevant_documents(self, query):
        # Implement the retrieval logic as before
        # ...

# Initialize the custom retriever
if 'custom_retriever' not in globals():
    custom_retriever = CustomHybridRetriever(vectorstores)

# Create the hybrid_chain using the custom retriever
if 'hybrid_chain' not in globals():
    hybrid_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=custom_retriever
    )

# Define the tools using the Tool class and include the input schema
tools = [
    Tool(
        name="query_dataframe",
        func=query_dataframe,
        description="Queries the liquidity DataFrame based on parameters.",
        args_schema=QueryDataFrameInput,
    ),
    Tool(
        name="visualize_fx_rates",
        func=visualize_fx_rates,
        description="Visualizes FX Rates data based on user parameters.",
        args_schema=VisualizeFXRatesInput,
    ),
    Tool(
        name="retrieve_documents",
        func=retrieve_documents,
        description="Retrieves relevant documents from the corpus.",
    )
]

# Create the agent using OpenAIFunctionsAgent
if 'agent_executor' not in globals():
    agent = OpenAIFunctionsAgent.from_llm_and_tools(
        llm=llm,
        tools=tools,
        system_prompt=system_prompt,
        extra_prompt_messages=[],
    )
    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        verbose=True,
        memory=memory,
    )

# Get user input from the Databricks widget
dbutils.widgets.text("Text Box", "", "Enter your query here")
user_input = dbutils.widgets.get("Text Box")

if user_input:
    # Append user's message to conversation history
    conversation_history.append({"role": "user", "content": user_input})

    # Run the agent with exception handling
    print("Assistant is typing...")
    try:
        assistant_response = agent_executor.run(
            input=user_input
        )
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        assistant_response = "I'm sorry, I encountered an error."

    # Append assistant's response to conversation history
    conversation_history.append({"role": "assistant", "content": assistant_response})

    # Clear the widget value for the next input
    dbutils.widgets.remove("Text Box")
    dbutils.widgets.text("Text Box", "", "Enter your query here")

    # Display the assistant's response
    print("\nAssistant:")
    print(assistant_response)
