# Initialize embeddings with batching and increased timeout
embeddings = OpenAIEmbeddings(
    openai_api_key=openai_api_key,
    batch_size=64,
    request_timeout=600
)

# Function to process documents and compute embeddings in parallel
def process_chunk_size(chunk_size):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)

    # Add metadata
    for idx, doc in enumerate(docs):
        doc.metadata['doc_id'] = doc.metadata.get('source', 'unknown')
        doc.metadata['chunk_id'] = idx
        doc.metadata['chunk_size'] = chunk_size

    # Compute embeddings in batches
    batch_size = 64
    texts = [doc.page_content for doc in docs]
    embeddings_list = []

    # Compute embeddings in batches
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        embeddings_batch = embeddings.embed_documents(batch_texts)
        embeddings_list.extend(embeddings_batch)

    # Create vector store and persist it
    vectorstore = Chroma(collection_name=f'docs_{chunk_size}', persist_directory=f'./chroma_{chunk_size}')
    vectorstore.add_embeddings(embeddings_list, docs)
    vectorstore.persist()

    return chunk_size, vectorstore

# Process each chunk size
chunk_sizes = [1024, 512]  # Reduced number of chunk sizes
vectorstores = {}

for chunk_size in chunk_sizes:
    chunk_size, vectorstore = process_chunk_size(chunk_size)
    vectorstores[chunk_size] = vectorstore
